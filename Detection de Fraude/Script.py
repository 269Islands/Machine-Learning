# -*- coding: utf-8 -*-
"""Detection de Fraude.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13pfW8k3ZQ3uetLcdLTQsWzBnPN2ZPUlc
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import warnings # Pour éviter les warnings
warnings.filterwarnings('ignore')

# Importation du dataset
data = pd.read_csv('creditcard.csv', sep=',')

data.head()

"""# **Exploration initiale**"""

# Dimension du dataset
data.shape

data.info()

data.Class.unique()

# Recodage de la variable cible en type qualitatif
data['Class'].replace([1,0], ['fraudée','non_fraudée'], inplace =True)

# Vérification du recodage de la variable cible
data.Class.unique()

data.dtypes.value_counts()

# Pourcentages des valeurs manquantes de chacune des variables de notre dataset
(data.isna().sum()/data.shape[0]).sort_values(ascending = False)*100

"""Notre dataset ne comporte aucune valeur maquante."""

#Analyse des lignes dupliquées
data.duplicated().sum()

"""# **Analyse exploratoire**"""

# Séléction des variables quantitatives
numCols = data.select_dtypes(include = 'number').columns.tolist()

"""### **Analyse univariée**"""

# Distribution de la variable cible
sns.countplot(x = 'Class',data = data)

plt.xlabel('Frauduleuses vs Non frauduleuses')
plt.ylabel("Nombre de cartes")

plt.show()

print(data['Class'].value_counts(normalize =True))

"""On peut remarquer que 99% des cartes ne sont pas frauduleuses. Donc,nos classes sont très déséquilibrées.
On s'attend alors à utiliser plus-tard, des métriques comme F1-Score, Précision ou Recall pour évaluer la performance de notre modèle.
"""

# Audit de l'unique variable qualitative Class
data.Class.describe()

# Audit des variables numériques
data[numCols].describe().T

"""Le résumé ci-dessus nous fait savoir que le montant maximal est de 25691... Toutefois, il est à signaler que le dataset a subi une transformation PCA."""

#Distribution des variables quantitatives
for col in numCols:
    plt.subplot(321)
    sns.distplot(data[col])

    plt.subplot(322)
    data[col].plot.box(figsize=(16,10))
    plt.suptitle('')
    plt.show()

"""On peut remarquer que toutes les variables numériques sont asymétriques, de plus elles renferment beaucoup de valeurs abérrantes.

### **Analyse bivariée**
"""

data.Class.value_counts()

# Création de sous ensemble de clients possédant des cartes frauduleuses et non frauduleuses.
Fraude = data[data['Class']== 'fraudée']
Non_Fraude = data[data['Class']== 'non_fraudée']

# Relation entre la variable cible et les variables exoplicatives
for col in numCols:
    plt.figure(figsize =(10,8))
    sns.distplot(Fraude[col], label='Oui')
    sns.distplot(Non_Fraude[col], label='Non')
    plt.legend()

"""On peut remarquer que certaines variables comme V4, V11, V15, V13... présentent des proportions de cartes frauduleuses assez importantes.

# **Prétraitement**
"""

# Suppression des lignes dupliquées
data = data.drop_duplicates()

# Suppression de la variable Time car non pertinente
df = data.drop("Time", axis = 1)
df.head()

numCols

"""## **Remplacement des valeurs aberrantes (outliers)**"""

#définition d'une fonction de suppression de valeurs aberrantes
def replace_outliers(df, numVar):
  Q1 = np.percentile(df[numVar], 25)
  Q3 = np.percentile(df[numVar], 75)
  IQR = Q3 - Q1
  min = Q1 - 1.5*IQR
  max = Q3 + 1.5*IQR
  #dataframeOut = dataframe.copy()
  df.loc[df[numVar] < min , numVar] = min
  df.loc[df[numVar] > max, numVar] = max #

for col in numCols:
    replace_outliers(df, col)

#Distribution des variables quantitatives pour vérifier les outliers
for col in numCols:
    plt.subplot(321)
    sns.distplot(df[col])

    plt.subplot(322)
    df[col].plot.box(figsize=(16,10))
    plt.suptitle('')
    plt.show()

"""## **Encodage de la variable cible**"""

df.head()

df_col = pd.get_dummies(df.Class,drop_first= True)
df_col.head()

df_col.value_counts()

# Renommage de la variable cible
df_col.columns = ['Cible']

# Concaténation
df_clean = pd.concat([df.drop("Class", axis = 1),df_col],axis = 1)

# Appercu
df_clean.head()

"""## **Division de la base en train/test (80 - 20)**"""

#séparation de la variable cible des variables explicatives
X = df_clean.drop("Cible" , axis =  1)
y = df_clean["Cible"]

#importation de la fonction de découpage (train_test_split)
from sklearn.model_selection import train_test_split

#découpage 80 - 20
X_train , X_test, y_train, y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42 )

print("Dimensions X_train :", X_train.shape)
print("Dimensions y_train :", y_train.shape)
print("Dimensions X_test :", X_test.shape)
print("Dimensions y_test :", y_test.shape)

X_train.head()

"""# **Modélisation**

## **Entrainement standard**
"""

#importation du f1_score
from sklearn.metrics import f1_score
#importation du la fonction time()
from time import time

#Définition de fonction
def modelisation(ModelsList , X_train_scaled , y_train , X_test_scaled , y_test ):
  scores = []
  for mod in ModelsList :
    try :
      model = mod(random_state = 42) #instanciation avec random_state
    except TypeError :
      model = mod() #instanciation sans random_state
    debut = time() #calcul du temps de début
    model.fit(X_train_scaled , y_train) #entrainement
    fin = time() #calcul du temps de fin
    y_pred_train = model.predict(X_train_scaled) #calcul des prédictions par rapport aux données d'entrainement
    y_pred_test = model.predict(X_test_scaled) #calcul des prédictions par rapport aux données de test
    scores.append({"Modeles" : type(model).__name__ ,
                   "temps_apprentissage" : fin - debut,
                   "f1_train_score" : f1_score(y_train , y_pred_train) ,
                   "f1_test_score" : f1_score(y_test , y_pred_test)})
  return pd.DataFrame(scores)

#importation des algorithmes
  from sklearn.linear_model import LogisticRegression
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.svm import SVC
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier, GradientBoostingClassifier

#liste de modeles
ModelsList = [RandomForestClassifier , AdaBoostClassifier, GradientBoostingClassifier,
              DecisionTreeClassifier , KNeighborsClassifier, LogisticRegression , SVC]

#premier essai de modélisation
scores = modelisation(ModelsList ,X_train, y_train , X_test , y_test)
scores.set_index("Modeles" , inplace = True)
scores

"""On peut remarquer que le modèle LogisticRegression est le meilleur modèle."""

#normalisation du temps_apprentissage
scores["temps_apprentissage"] = scores["temps_apprentissage"]/scores["temps_apprentissage"].max()
scores

#visualisation
scores.plot.bar(figsize=(12,10))
plt.show()

"""## **Optimisation de l'algorithme de la regression**

Il n'est pas vraiment utile d'optimiser le modèle car ayant une performance de 99%.
Cependant, on peut faire l'optimisation en guise d'exemple pour un prochain projet présentant des performances moins des 99%.
"""

from sklearn.model_selection import GridSearchCV

# Sélection d'hyperparamètres
tuned_parameters = [{'penalty': ['l1','l2'],'C':np.logspace(-3,3,7), 'solver': ['newton-cg','lbfgs','liblinear']}]

model_grid = GridSearchCV(estimator=LogisticRegression(),param_grid=tuned_parameters, scoring ='f1',cv =3)
model_grid.fit(X_train, y_train)

print("meilleur score:",model_grid.best_score_)
print("meilleurs parametres:",model_grid.best_params_)

best_model = LogisticRegression(C =10, penalty='l2', solver = 'lbfgs')
best_model.fit(X_train, y_train)
pred_best = best_model.predict(X_test)

# Calcul de la précision
print("Précision:",precision_score(y_test, pred_best)*100)

# Calcul du recall
print("Recall:",recall_score(y_test, pred_best)*100)

# Calcul du F1-Score
print("F1-Score:",f1_score(y_test, pred_best)*100)

from sklearn.metrics import plot_roc_curve, roc_auc_score
plot_roc_curve(best_model,X_test,y_test) # Coube ROC

#predictions_prob = best_model.predict(X_test) # La prediction en probabilite
auc = roc_auc_score(y_test, pred_best) # oubien auc = roc_auc_score(y_test, predictions_prob)

print("Area Under Curve :", auc)

"""## **Enregistrement du modèle**

"""

import joblib

# Enregistrement de notre modèle
joblib.dump(value=best_model, filename ='Detection de cartes frauduleuses.pkl')

## Chargement du modèle pour de nouvelles prédictions en cas
load_model = joblib.load(filename ='Detection de cartes frauduleuses.pkl')

